{"backend_state":"init","connection_file":"/projects/616bebf1-83bb-4da8-a0be-bad824c72dfe/.local/share/jupyter/runtime/kernel-8f38b56f-a670-490b-97e8-75c78d5f8439.json","kernel":"python3-ubuntu","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":2},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":23,"id":"87da95","input":"import os\n\nfrom skimage.io import imread\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n\n\nimgs = []\nfor f in sorted(os.listdir('data/day6/alignment/timeseries3_withbeads')):\n    if f.endswith('.tif'):\n        img = imread(os.path.join('data/day6/alignment/timeseries3_withbeads', f))\n        imgs.append(img.astype(np.float32))\n\nimport scipy.ndimage as ndi\nfrom skimage.feature import peak_local_max\n\n# we detect maxima in an inverted Laplacian-of-Gaussian filtered image\n# this enhances blobs of a given size\npeaks2 = peak_local_max(-ndi.gaussian_laplace(imgs[1], 5), threshold_abs=3)\npeaks4 = peak_local_max(-ndi.gaussian_laplace(imgs[3], 5), threshold_abs=3)\npeaks6 = peak_local_max(-ndi.gaussian_laplace(imgs[5], 5), threshold_abs=3)\n\n\n\n# coordinates are in yx -> invert them to xy\npeaks2 = peaks2[:,::-1]\npeaks4 = peaks4[:,::-1]\npeaks6 = peaks6[:,::-1]\n\n\nfrom skimage.transform import warp, AffineTransform\n\n\n\nat2_4 = AffineTransform()\n\n# estimate from TP 1 to TP 5\nat2_4.estimate(np.array(peaks2), np.array(peaks4))\n\n\norb2 = ORB(fast_threshold=0.0005)\norb2.detect_and_extract(imgs[1])\n\norb6 = ORB(fast_threshold=0.0005)\norb6.detect_and_extract(imgs[5])\n\n\nfrom skimage.feature import match_descriptors, plot_matches\n\nmatches_1 = match_descriptors(orb2.descriptors, orb6.descriptors)\n\nkeypoints2 = orb2.keypoints[matches_1[:,0]]\nkeypoints6 = orb6.keypoints[matches_1[:,1]]\n\n# flip yx as above\nkeypoints2 = keypoints2[:,::-1]\nkeypoints6 = keypoints6[:,::-1]\n\nfrom skimage.measure import ransac\nfrom skimage.transform import AffineTransform\n\n# AffineTransform requires 3 corresponding points -> min_samples=3\n# with residual_threshold, we can set an upper bound for the error\ntransform, inliers = ransac((keypoints2, keypoints6), AffineTransform, min_samples=3, residual_threshold=10)\n\n\n\n\n\naxs[0].imshow(imgs[1])\naxs[0].plot(peaks2[:,0], peaks2[:,1], 'x', color='red')\naxs[0].set_title('TP 2')\naxs[1].imshow(warp(imgs[3], at2_4, clip=False))\naxs[1].plot([p[0] for p in at2_4.inverse(peaks4)], [p[1] for p in at2_4.inverse(peaks4)], 'x', color='red')\naxs[1].set_title('TP 4')\naxs[2].imshow(warp(imgs[5], transform, clip=False))\naxs[2].plot([p[0] for p in transform.inverse(peaks6)], [p[1] for p in transform.inverse(peaks6)], 'x', color='red')\naxs[2].set_title('TP 6')","output":{"0":{"data":{"text/plain":"Text(0.5, 1.0, 'TP 6')"},"exec_count":23,"output_type":"execute_result"},"1":{"data":{"image/png":"82476dee8d252776b2223aa98f55ddb966d9608e","text/plain":"<Figure size 864x288 with 3 Axes>"},"exec_count":23,"metadata":{"image/png":{"height":184,"width":710},"needs_background":"light"},"output_type":"execute_result"}},"pos":19,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"8eec19","input":"import os\nfrom skimage.io import imread\n\nimgs = []\nfor f in sorted(os.listdir('data/day6/alignment/timeseries2')):\n    if f.endswith('.tif'):\n        img = imread(os.path.join('data/day6/alignment/timeseries2', f))\n        imgs.append(img)\norb2 = ORB(fast_threshold=0.0005)\norb2.detect_and_extract(imgs[1])\n\norb4 = ORB(fast_threshold=0.0005)\norb4.detect_and_extract(imgs[3])\n\norb6 = ORB(fast_threshold=0.0005)\norb6.detect_and_extract(imgs[5])\n\nfig, axs = plt.subplots(ncols=3, figsize=(12,4))\naxs[0].imshow(imgs[1])\naxs[0].plot(orb2.keypoints[:,1], orb2.keypoints[:,0], 'x', color='red')\naxs[1].imshow(imgs[3])\naxs[1].plot(orb4.keypoints[:,1], orb4.keypoints[:,0], 'x', color='red')\naxs[2].imshow(imgs[5])\naxs[2].plot(orb6.keypoints[:,1], orb6.keypoints[:,0], 'x', color='red')\n\n\n\nfrom skimage.feature import match_descriptors, plot_matches\n\nmatches_1 = match_descriptors(orb2.descriptors, orb4.descriptors)\n\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[1], imgs[3], orb2.keypoints, orb4.keypoints, matches_1)\n\nmatches_2 = match_descriptors(orb4.descriptors, orb6.descriptors)\n\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[3], imgs[5], orb4.keypoints, orb6.keypoints, matches_2)\n\nkeypoints2 = orb2.keypoints[matches_1[:,0]]\nkeypoints4_1 = orb4.keypoints[matches_1[:,1]]\n\n# flip yx as above\nkeypoints2 = keypoints2[:,::-1]\nkeypoints4_1 = keypoints4_1[:,::-1]\n\nkeypoints4_2 = orb4.keypoints[matches_2[:,0]]\nkeypoints6 = orb6.keypoints[matches_2[:,1]]\n\n# flip yx as above\nkeypoints4_2 = keypoints4_2[:,::-1]\nkeypoints6 = keypoints6[:,::-1]\n\nfrom skimage.measure import ransac\nfrom skimage.transform import AffineTransform\n\n# AffineTransform requires 3 corresponding points -> min_samples=3\n# with residual_threshold, we can set an upper bound for the error\ntransform_1, inliers = ransac((keypoints2, keypoints4_1), AffineTransform, min_samples=3, residual_threshold=10)\n\n# get indices of inlying keypoints\ninlier_idxs = np.nonzero(inliers)[0]\n\n# plot filtered matches\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[1], imgs[3], keypoints2[:,::-1], keypoints4_1[:,::-1],\n             np.column_stack((inlier_idxs, inlier_idxs)), matches_color='b')\n\ntransform, inliers = ransac((keypoints4_2, keypoints6), AffineTransform, min_samples=3, residual_threshold=10)\n\n# get indices of inlying keypoints\ninlier_idxs = np.nonzero(inliers)[0]\n\n# plot filtered matches\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[3], imgs[5], keypoints4_2[:,::-1], keypoints6[:,::-1],\n             np.column_stack((inlier_idxs, inlier_idxs)), matches_color='b')\n\n\nfig, axs = plt.subplots(ncols=3, figsize=(12,4))\n\naxs[0].imshow(imgs[1])\naxs[0].set_title('TP 2')\naxs[1].imshow(warp(imgs[3], transform_1, clip=False))\naxs[1].set_title('TP 4')\naxs[2].imshow(warp(imgs[5], transform+transform_1, clip=False))\naxs[2].set_title('TP 6')","output":{"0":{"data":{"text/plain":"Text(0.5, 1.0, 'TP 6')"},"exec_count":25,"output_type":"execute_result"},"1":{"data":{"image/png":"aa0a0a4260ee557e5fdf1efe10c4cf58c9c1b0a7","text/plain":"<Figure size 864x288 with 3 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":165,"width":710},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"d2a14d0b62bd9138c316e8fc3bc9a272c99b1b24","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":252,"width":705},"needs_background":"light"},"output_type":"execute_result"},"3":{"data":{"image/png":"dac1cdb9cf0d0ffc0ea07152486c83dfc05ed4a8","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":252,"width":705},"needs_background":"light"},"output_type":"execute_result"},"4":{"data":{"image/png":"6c17ea18296695c1265c09b70ae2b6f7bbcddc23","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":252,"width":705},"needs_background":"light"},"output_type":"execute_result"},"5":{"data":{"image/png":"f19bee6edc858b04f20419845d8fc4b0f2305b8b","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":252,"width":705},"needs_background":"light"},"output_type":"execute_result"},"6":{"data":{"image/png":"2538f892b90cc54ef0c8b51fa2b9ae89b5a02e4d","text/plain":"<Figure size 864x288 with 3 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":177,"width":710},"needs_background":"light"},"output_type":"execute_result"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":25,"id":"b01706","input":"from skimage.io import imread\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.feature import match_template\n\n# read the images and convert them to floating point (otherwise, filters might generate overflows)\ntemplate = imread('data/day6/alignment/templatematching/template2.tif').astype(np.float64)\nimg = imread('data/day6/alignment/templatematching/image.tif').astype(np.float64)\n\n\n#plt.figure()\n#plt.imshow(img, cmap='gray')\n\nplt.figure()\nplt.imshow(template, cmap='gray')\n\nfiltered = match_template(img, template, pad_input=True)\n#plt.figure()\n#plt.imshow(filtered)\n\nmax_index = np.argmax(filtered)\n\n\ncoords = np.unravel_index(max_index, img.shape)\n\n#match = img[coords[0]-50:coords[0]+50, coords[1]-50:coords[1]+50]\n#plt.figure()\n#plt.imshow(match, cmap='gray')\n\nfrom matplotlib.patches import Rectangle\n\nplt.imshow(img, cmap='gray')\n\n# note that Rectangle uses xy coordinates, but our result is yx\nrec = Rectangle([coords[1]-50, coords[0]-50], 100, 100, fill=None, color='red')\nplt.gca().add_artist(rec)","output":{"0":{"data":{"text/plain":"<matplotlib.patches.Rectangle at 0x7fc1d6eece50>"},"exec_count":25,"output_type":"execute_result"},"1":{"data":{"image/png":"d788071fb8c2c8a4173ec839e0d29a367e2968de","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":25,"metadata":{"image/png":{"height":414,"width":547},"needs_background":"light"},"output_type":"execute_result"}},"pos":1,"type":"cell"}
{"cell_type":"code","exec_count":27,"id":"0a3600","input":"from skimage.feature import ORB\nimport os\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\n\n\nimgs = []\nfor f in sorted(os.listdir('data/day6/alignment/timeseries')):\n    if f.endswith('.tif'):\n        img = imread(os.path.join('data/day6/alignment/timeseries', f))\n        imgs.append(img)\n        \norb2 = ORB(fast_threshold=0.0005)\norb2.detect_and_extract(imgs[1])\n\norb4 = ORB(fast_threshold=0.0005)\norb4.detect_and_extract(imgs[3])\n\norb6 = ORB(fast_threshold=0.0005)\norb6.detect_and_extract(imgs[5])\n\nfig, axs = plt.subplots(ncols=3, figsize=(12,4))\naxs[0].imshow(imgs[1])\naxs[0].plot(orb2.keypoints[:,1], orb2.keypoints[:,0], 'x', color='red')\naxs[1].imshow(imgs[3])\naxs[1].plot(orb4.keypoints[:,1], orb4.keypoints[:,0], 'x', color='red')\naxs[2].imshow(imgs[5])\naxs[2].plot(orb6.keypoints[:,1], orb6.keypoints[:,0], 'x', color='red')\n\n\n\nfrom skimage.feature import match_descriptors, plot_matches\n\nmatches_1 = match_descriptors(orb2.descriptors, orb4.descriptors)\n\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[1], imgs[3], orb2.keypoints, orb4.keypoints, matches_1)\n\nmatches_2 = match_descriptors(orb4.descriptors, orb6.descriptors)\n\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[3], imgs[5], orb4.keypoints, orb6.keypoints, matches_2)\n\nkeypoints2 = orb2.keypoints[matches_1[:,0]]\nkeypoints4_1 = orb4.keypoints[matches_1[:,1]]\n\n# flip yx as above\nkeypoints2 = keypoints2[:,::-1]\nkeypoints4_1 = keypoints4_1[:,::-1]\n\nkeypoints4_2 = orb4.keypoints[matches_2[:,0]]\nkeypoints6 = orb6.keypoints[matches_2[:,1]]\n\n# flip yx as above\nkeypoints4_2 = keypoints4_2[:,::-1]\nkeypoints6 = keypoints6[:,::-1]\n\nfrom skimage.measure import ransac\nfrom skimage.transform import AffineTransform\n\n# AffineTransform requires 3 corresponding points -> min_samples=3\n# with residual_threshold, we can set an upper bound for the error\ntransform_1, inliers = ransac((keypoints2, keypoints4_1), AffineTransform, min_samples=3, residual_threshold=10)\n\n# get indices of inlying keypoints\ninlier_idxs = np.nonzero(inliers)[0]\n\n# plot filtered matches\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[1], imgs[3], keypoints2[:,::-1], keypoints4_1[:,::-1],\n             np.column_stack((inlier_idxs, inlier_idxs)), matches_color='b')\n\ntransform, inliers = ransac((keypoints4_2, keypoints6), AffineTransform, min_samples=3, residual_threshold=10)\n\n# get indices of inlying keypoints\ninlier_idxs = np.nonzero(inliers)[0]\n\n# plot filtered matches\nplt.figure(figsize=(12,4))\nplot_matches(plt.gca(), imgs[3], imgs[5], keypoints4_2[:,::-1], keypoints6[:,::-1],\n             np.column_stack((inlier_idxs, inlier_idxs)), matches_color='b')\n\nfig, axs = plt.subplots(ncols=3, figsize=(12,4))\n\naxs[0].imshow(imgs[1])\naxs[0].set_title('TP 2')\naxs[1].imshow(warp(imgs[3], transform_1, clip=False))\naxs[1].set_title('TP 4')\naxs[2].imshow(warp(imgs[5], transform+transform_1, clip=False))\naxs[2].set_title('TP 6')","output":{"0":{"data":{"text/plain":"Text(0.5, 1.0, 'TP 6')"},"exec_count":27,"output_type":"execute_result"},"1":{"data":{"image/png":"bca9ac271b82ead17d37dd8235a00f9fffea5c13","text/plain":"<Figure size 864x288 with 3 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":187,"width":710},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"66a995426e373dbf41371295001e39516a864eae","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":252,"width":612},"needs_background":"light"},"output_type":"execute_result"},"3":{"data":{"image/png":"f03b78eebeb3d2ce95578031f9389a3c5abb204f","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":252,"width":612},"needs_background":"light"},"output_type":"execute_result"},"4":{"data":{"image/png":"cb290433c95a31c9447c7e1d6b8b8f56c7c1cbaf","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":252,"width":612},"needs_background":"light"},"output_type":"execute_result"},"5":{"data":{"image/png":"6d3a0f8a3da2cf2a57bdcdea5316e193c0b9d23f","text/plain":"<Figure size 864x288 with 1 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":252,"width":612},"needs_background":"light"},"output_type":"execute_result"},"6":{"data":{"image/png":"242bd5e1e6a8403ef0159006d3550219ee819668","text/plain":"<Figure size 864x288 with 3 Axes>"},"exec_count":27,"metadata":{"image/png":{"height":199,"width":710},"needs_background":"light"},"output_type":"execute_result"}},"pos":14,"type":"cell"}
{"cell_type":"code","exec_count":34,"id":"08a07a","input":"#m = np.argmax(filters)\n#np.unravel_index(m, filters.shape)","output":{"0":{"data":{"text/plain":"(4, 332, 761)"},"exec_count":34,"output_type":"execute_result"}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":36,"id":"18e6c2","input":"from skimage.io import imread\nfrom  skimage.transform import rotate\n\n\ntemplate = imread('data/day6/alignment/templatematching/template3.tif').astype(np.float64)\nimg = imread('data/day6/alignment/templatematching/image.tif').astype(np.float64)\n\n#plt.figure()\n#plt.imshow(img, cmap='gray')\n\nplt.figure()\nplt.imshow(template, cmap='gray')\n\nmax_value = -np.inf\nmax_index = 0\nrotated_time = 0\nnum_rotations = 36\nfilters = np.zeros((num_rotations,) + img.shape)\nfor i, rotation in enumerate(range(0, 360, 10)):\n    filters[i] = match_template(img, rotate(template, rotation), pad_input=True)\n    temp = np.argmax(filters[i])\n    temp_value = np.max(filters[i])\n    if temp_value > max_value:\n        max_value = temp_value\n        max_index = temp\n        rotated_time = i\n\n\ncoords = np.unravel_index(max_index, img.shape)\n\n\nfrom matplotlib.patches import Rectangle\nplt.figure()\nplt.imshow(img, cmap='gray')\n\n# note that Rectangle uses xy coordinates, but our result is yx\nrec = Rectangle([coords[1]-50, coords[0]-50], 100, 100, fill=None, color='red')\nplt.gca().add_artist(rec)\n","output":{"0":{"data":{"text/plain":"<matplotlib.patches.Rectangle at 0x7fc1d676cbb0>"},"exec_count":36,"output_type":"execute_result"},"1":{"data":{"image/png":"7f2d4a8986556c2f433f4f6131997e7af6632b14","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":36,"metadata":{"image/png":{"height":413,"width":421},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"d788071fb8c2c8a4173ec839e0d29a367e2968de","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":36,"metadata":{"image/png":{"height":414,"width":547},"needs_background":"light"},"output_type":"execute_result"}},"pos":3,"type":"cell"}
{"cell_type":"code","exec_count":37,"id":"2634e3","input":"from skimage.io import imread\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.feature import match_template\n\n\nimg1 = imread('data/day6/alignment/stitching/tile1.tif').astype(np.float64)\nimg2 = imread('data/day6/alignment/stitching/tile2.tif').astype(np.float64)\nimg3 = imread('data/day6/alignment/stitching/tile3.tif').astype(np.float64)\n\n\nplt.figure()\nplt.imshow(img1)\n\nplt.figure()\nplt.imshow(img2)\nplt.figure()\nplt.imshow(img3)\nfrom skimage.registration import phase_cross_correlation\n\nshift_1, corr, _ = phase_cross_correlation(img1, img2)\n\nshift_2, corr, _ = phase_cross_correlation(img2, img3)\n\nshift = shift_1 + shift_2\nfrom skimage.transform import warp, AffineTransform\n#######################\nshift_1 = shift_1.astype(int) # we will be using this to index into our images, so it has to be int, not float\nshift = shift.astype(int)\ntotal_size = np.array(img1.shape) + np.abs(shift)\n\n# get the offset if shift is negative\n# np.iinfo(np.int).min gives us the minimal possible integer value\n# the folowing code will ignore all shifts > 0 (in which case we do not need to worry about the offset)\noffset = -np.clip(shift_1, np.iinfo(int).min, 0)\n\n# make an empty image\nfused = np.zeros(total_size)\nweights_1 = np.zeros(total_size)\n\n# construct transform for img1, this is just the offset\ntr1 = AffineTransform(translation=offset[::-1])\n# construct transform for img2, this is the shift plus the offset (note that you can just concatenate AffineTransforms with +)\ntr2 = AffineTransform(translation=offset[::-1]) + AffineTransform(translation=shift_1[::-1])\n\n# add the images to fused\nfused += warp(img1, tr1.inverse, output_shape=total_size)\nfused += warp(img2, tr2.inverse, output_shape=total_size)\n\n# add ones as weights in places where the images are fused (Note np.ones_like() creates an image filled with ones with the same size as the input)\nweights_1 += warp(np.ones_like(img1), tr1.inverse, output_shape=total_size)\nweights_1 += warp(np.ones_like(img2), tr2.inverse, output_shape=total_size)\n\nfused[weights_1!=0] /= weights_1[weights_1!=0]\nfused_1 = fused\nplt.figure()\nplt.imshow(fused_1)\n######################\n\nshift_2 = shift_2.astype(int) # we will be using this to index into our images, so it has to be int, not float\n\ntotal_size = np.array(img1.shape) + np.abs(shift)\n\n# get the offset if shift is negative\n# np.iinfo(np.int).min gives us the minimal possible integer value\n# the folowing code will ignore all shifts > 0 (in which case we do not need to worry about the offset)\noffset = -np.clip(shift_2, np.iinfo(int).min, 0)\n\n# make an empty image\nfused = np.zeros(total_size)\nweights_2 = np.zeros(total_size)\n\n# construct transform for img1, this is just the offset\ntr1 = AffineTransform(translation=offset[::-1])\n# construct transform for img2, this is the shift plus the offset (note that you can just concatenate AffineTransforms with +)\ntr2 = AffineTransform(translation=offset[::-1]) + AffineTransform(translation=shift_2[::-1])\n\n# add the images to fused\nfused += warp(img2, tr1.inverse, output_shape=total_size)\nfused += warp(img3, tr2.inverse, output_shape=total_size)\n\n# add ones as weights in places where the images are fused (Note np.ones_like() creates an image filled with ones with the same size as the input)\nweights_2 += warp(np.ones_like(img2), tr1.inverse, output_shape=total_size)\nweights_2 += warp(np.ones_like(img3), tr2.inverse, output_shape=total_size)\n\nfused[weights_2!=0] /= weights_2[weights_2!=0]\nfused_2 = fused\n\n\nplt.figure()\nplt.imshow(fused_2)\n\n#########################\n\nshift_3, corr, _ = phase_cross_correlation(fused_1, fused_2)\n\nshift_3 = shift_3.astype(int) # we will be using this to index into our images, so it has to be int, not float\n\ntotal_size = np.array(fused_2.shape)\n\n# get the offset if shift is negative\n# np.iinfo(np.int).min gives us the minimal possible integer value\n# the folowing code will ignore all shifts > 0 (in which case we do not need to worry about the offset)\noffset = -np.clip(shift_3, np.iinfo(int).min, 0)\n\n# make an empty image\nfused = np.zeros(total_size)\nweights = np.zeros(total_size)\n\n# construct transform for img1, this is just the offset\ntr1 = AffineTransform(translation=offset[::-1])\n# construct transform for img2, this is the shift plus the offset (note that you can just concatenate AffineTransforms with +)\ntr2 = AffineTransform(translation=offset[::-1]) + AffineTransform(translation=shift_3[::-1])\n\n# add the images to fused\nfused += warp(fused_1, tr1.inverse, output_shape=total_size)\nfused += warp(fused_2, tr2.inverse, output_shape=total_size)\n\n# add ones as weights in places where the images are fused (Note np.ones_like() creates an image filled with ones with the same size as the input)\nweights += warp(np.ones_like(fused_1), tr1.inverse, output_shape=total_size)\nweights += warp(np.ones_like(img3), tr2.inverse, output_shape=total_size)\n\n\n\nfused[weights!=0] = fused[weights!=0] / weights[weights!=0]\n\nfused_3 = fused\n\n\nplt.figure()\nplt.imshow(fused_3)","output":{"0":{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fcf3088b370>"},"exec_count":37,"output_type":"execute_result"},"1":{"data":{"image/png":"43ea8cbd8f24626d3b7b3033cf81073f79218a88","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":37,"metadata":{"image/png":{"height":414,"width":422},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"a9250889e6e20592d62b9cd72b3ad7f3f95de2ca","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":37,"metadata":{"image/png":{"height":414,"width":422},"needs_background":"light"},"output_type":"execute_result"},"3":{"data":{"image/png":"cc2ed55e159de1ca8983b8e5f68bedde2b9d2d7c","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":37,"metadata":{"image/png":{"height":414,"width":422},"needs_background":"light"},"output_type":"execute_result"},"4":{"data":{"image/png":"d000191e4aadbd8834eef8b3d234e27da5f90da6","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":37,"metadata":{"image/png":{"height":414,"width":581},"needs_background":"light"},"output_type":"execute_result"},"5":{"data":{"image/png":"e208612aba1bedb4cb0194e2eee7ab1e6baf13b7","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":37,"metadata":{"image/png":{"height":414,"width":581},"needs_background":"light"},"output_type":"execute_result"},"6":{"data":{"image/png":"42e431ad3191125d1ffe1637d54fdce7530aa5af","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":37,"metadata":{"image/png":{"height":414,"width":581},"needs_background":"light"},"output_type":"execute_result"}},"pos":8,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"294cbb","input":"import os\n\nfrom skimage.io import imread\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n\n\nimgs = []\nfor f in sorted(os.listdir('data/day6/alignment/timeseries/withbeads')):\n    if f.endswith('.tif'):\n        img = imread(os.path.join('data/day6/alignment/timeseries/withbeads', f))\n        imgs.append(img.astype(np.float32))\n\nimport scipy.ndimage as ndi\nfrom skimage.feature import peak_local_max\n\n# we detect maxima in an inverted Laplacian-of-Gaussian filtered image\n# this enhances blobs of a given size\npeaks2 = peak_local_max(-ndi.gaussian_laplace(imgs[1], 5), threshold_abs=3)\npeaks4 = peak_local_max(-ndi.gaussian_laplace(imgs[3], 5), threshold_abs=3)\npeaks6 = peak_local_max(-ndi.gaussian_laplace(imgs[5], 5), threshold_abs=3)\n\n\n\n# coordinates are in yx -> invert them to xy\npeaks2 = peaks2[:,::-1]\npeaks4 = peaks4[:,::-1]\npeaks6 = peaks6[:,::-1]\n\n\nfrom skimage.transform import warp, AffineTransform\n \n\n\nat2_4 = AffineTransform()\nat4_6 = AffineTransform()\n\n# estimate from TP 1 to TP 5\nat2_4.estimate(np.array(peaks2), np.array(peaks4))\n\n# estimate from TP 5 to TP 10\nat4_6.estimate(np.array(peaks4), np.array(peaks6))\n\n# 1 to 10: combine both transforms\nat2_6 = at2_4 + at4_6\n\n\n\nfig, axs = plt.subplots(ncols=3, figsize=(12,4))\naxs[0].imshow(imgs[1])\naxs[0].plot(peaks2[:,0], peaks2[:,1], 'x', color='red')\naxs[0].set_title('TP 2')\naxs[1].imshow(warp(imgs[3], at2_4, clip=False))\naxs[1].plot([p[0] for p in at2_4.inverse(peaks4)], [p[1] for p in at2_4.inverse(peaks4)], 'x', color='red')\naxs[1].set_title('TP 4')\naxs[2].imshow(warp(imgs[5], at2_6, clip=False))\naxs[2].plot([p[0] for p in at2_6.inverse(peaks6)], [p[1] for p in at2_6.inverse(peaks6)], 'x', color='red')\naxs[2].set_title('TP 6')","output":{"0":{"data":{"text/plain":"Text(0.5, 1.0, 'TP 6')"},"exec_count":5,"output_type":"execute_result"},"1":{"data":{"image/png":"9f447d1fa65572e3eaf04cabeb0eed6935fcdbf9","text/plain":"<Figure size 864x288 with 3 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":199,"width":710},"needs_background":"light"},"output_type":"execute_result"}},"pos":12,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"4b669d","input":"from skimage.io import imread\nfrom  skimage.transform import rotate\n\n\nimg1 = imread('data/day6/alignment/stitching/tile2.tif').astype(np.float64)\nimg2 = imread('data/day6/alignment/stitching/tile3.tif').astype(np.float64)\n\nplt.figure()\nplt.imshow(img1)\n\nplt.figure()\nplt.imshow(img2)\n\nfrom skimage.registration import phase_cross_correlation\n\nshift, corr, _ = phase_cross_correlation(img1, img2)\nshift\n\nfrom skimage.transform import warp, AffineTransform\n\n# construct transform, note that we have to invert the coordinates to xy\nat = AffineTransform(translation=shift[::-1])\n\n# warp img2, note that warp requires the inverse of at\ntransformed = warp(img2, at.inverse)\n\nshift = shift.astype(int) # we will be using this to index into our images, so it has to be int, not float\n\ntotal_size = np.array(img1.shape) + np.abs(shift)\n\n# get the offset if shift is negative\n# np.iinfo(np.int).min gives us the minimal possible integer value\n# the folowing code will ignore all shifts > 0 (in which case we do not need to worry about the offset)\noffset = -np.clip(shift, np.iinfo(int).min, 0)\n\n# make an empty image\nfused = np.zeros(total_size)\nweights = np.zeros(total_size)\n\n# construct transform for img1, this is just the offset\ntr1 = AffineTransform(translation=offset[::-1])\n# construct transform for img2, this is the shift plus the offset (note that you can just concatenate AffineTransforms with +)\ntr2 = AffineTransform(translation=offset[::-1]) + AffineTransform(translation=shift[::-1])\n\n# add the images to fused\nfused += warp(img1, tr1.inverse, output_shape=total_size)\nfused += warp(img2, tr2.inverse, output_shape=total_size)\n\n# add ones as weights in places where the images are fused (Note np.ones_like() creates an image filled with ones with the same size as the input)\nweights += warp(np.ones_like(img1), tr1.inverse, output_shape=total_size)\nweights += warp(np.ones_like(img2), tr2.inverse, output_shape=total_size)\n\nfused[weights!=0] /= weights[weights!=0]\nplt.figure()\nplt.imshow(fused)","output":{"0":{"data":{"text/plain":"<matplotlib.image.AxesImage at 0x7fcf30f523d0>"},"exec_count":5,"output_type":"execute_result"},"1":{"data":{"image/png":"a9250889e6e20592d62b9cd72b3ad7f3f95de2ca","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":414,"width":422},"needs_background":"light"},"output_type":"execute_result"},"2":{"data":{"image/png":"cc2ed55e159de1ca8983b8e5f68bedde2b9d2d7c","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":414,"width":422},"needs_background":"light"},"output_type":"execute_result"},"3":{"data":{"image/png":"fccbc772284d26e145db70897e576b5640a667bd","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":5,"metadata":{"image/png":{"height":414,"width":488},"needs_background":"light"},"output_type":"execute_result"}},"pos":6,"type":"cell"}
{"cell_type":"code","exec_count":6,"id":"2edec9","input":"import os\nfrom skimage.io import imread\nimport matplotlib.pyplot as plt\n\n\nimgs = []\nfor f in sorted(os.listdir('data/day6/alignment/timeseries')):\n    if f.endswith('.tif'):\n        img = imread(os.path.join('data/day6/alignment/timeseries', f))\n        imgs.append(img)\n\nimport json\n\nwith open('data/day6/alignment/timeseries/cell_coordinates.json') as fd:\n    coords = json.load(fd)\n\n\nat2_4 = AffineTransform()\nat4_6 = AffineTransform()\n\n# estimate from TP 1 to TP 5\nat2_4.estimate(np.array(coords[1]), np.array(coords[3]))\n\n# estimate from TP 5 to TP 10\nat4_6.estimate(np.array(coords[3]), np.array(coords[5]))\n\n# 1 to 10: combine both transforms\nat2_6 = at2_4 + at4_6\n\nfig, axs = plt.subplots(ncols=3, figsize=(16,4))\naxs[0].imshow(imgs[1])\naxs[0].plot([p[0] for p in coords[1]], [p[1] for p in coords[1]], 'x', color='red')\naxs[0].set_title('TP 2')\n\n# plot tp 5 aligned to first image\n# note: we need the transform 1->5 to align 5 to 1 (the inverse of 5->1)\naxs[1].imshow(warp(imgs[3], at2_4, clip=False))\n# plot coordinates from tp 5 transformed to tp 1: we need inverse of at1_5\naxs[1].plot([p[0] for p in at2_4.inverse(coords[3])], [p[1] for p in at2_4.inverse(coords[3])], 'x', color='red')\naxs[1].set_title('TP 4')\n\n# plot tp 10 aligned\naxs[2].imshow(warp(imgs[5], at2_6, clip=False))\naxs[2].plot([p[0] for p in at2_6.inverse(coords[5])], [p[1] for p in at2_6.inverse(coords[5])], 'x', color='red')\naxs[2].set_title('TP 6')","output":{"0":{"data":{"text/plain":"Text(0.5, 1.0, 'TP 6')"},"exec_count":6,"output_type":"execute_result"},"1":{"data":{"image/png":"b408e970f81531141eeb50d9a263e6f107287603","text/plain":"<Figure size 1152x288 with 3 Axes>"},"exec_count":6,"metadata":{"image/png":{"height":250,"width":933},"needs_background":"light"},"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"markdown","id":"07edb5","input":"* Perform descriptor-based alignment on 3 other timepoints in ```data/day6/alignment/timeseries```","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"243313","input":"## Tasks Stitching\n  * calcualte the shift between ```data/day6/alignment/stitching/tile2.tif``` and ```data/day6/alignment/stitching/tile3.tif``` and create a fused image\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"24f2f2","input":"```data/day6/alignment/timeseries2``` contains 10 timepoints of shifted images from another sample of DAPI-stained nuclei.  \n\nFind a strategy to align 3 timepoints of your choice. You can:\n\n* Manually select nuclei\n* Use ORB+RANSAC descriptor-based registration\n* The shifts consist almost entirely of translations, so you could also use ```register_translation``` to align them\n\n","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"4f6c4b","input":"## Tasks Template Matching:\n\n  * find the template ```data/day6/alignment/templatematching/template2.tif``` in the image ```data/day6/alignment/templatematching/image.tif```\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"94ee1f","input":"## Tasks Interest Point-based Alignment\nIn the handout, we have aligned timepoints 1,5 and 10.\nTry to align other timepoints of ```data/day6/alignment/timeseries``` and **plot the results**\n  * play around with the examples a bit\n  * use the coordinates in ```data/day6/alignment/timeseries/cell_coordinates.json``` to align 3 other timepoints of your choice","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"b43245","input":"We can also use ```match_template``` to find a rotated version of our template by trying multiple rotations and see which one matches best:\n\n  * find the template ```data/day6/alignment/templatematching/template3.tif```. Note that the template has been rotated. You can use ```skimage.transform.rotate``` to generate a rotated version.\n  \n      * try many rotations (go in 10 degree steps) to find both the location and rotation angle  \n      You could use \n      ```python\n      num_rotations = 36\n      filters = np.zeros((num_rotations,) + img.shape)\n      for i, rotation in enumerate(range(0, 360, 10)):\n            filters[i] = match_template(img, rotate(template, rotation), pad_input=True)\n      ```\n      and then find the 3-dimensional maximum in ```filters```. the first dimension should correspond to the rotation angle *index*, the others to the location\n\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"c1bee2","input":"  * **Optional Challenge:** Use the shifts between tile1.tif, tile2.tif and tile3.tif to fuse all 3 images.\n  \n  * calculate the shifts between tile1 and tile2 again (see handout), combine the shifts from 1-2 and 2-3 to get the shift from 1-3\n  * with the shifts relative to tile 1, calculate the offset in case any of the shifts become negative\n  * calculate the size needed for the fused image (size of tile 1 + the maximum absolute value in each dimension)\n  * calculate the weighted average fusion as you did with 2 images","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"db890b","input":"```data/day6/alignment/timeseries3_withbeads``` contains 10 timepoints of shifted images from another sample of DAPI-stained nuclei, this time with 5 fiducial beads. \n\n* Align 3 timepoints of your choice and align them by detecting the beads and registering the coordinates.\n\n","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"dc9fe7","input":"## OPTIONAL: some more practice on other images\n\n","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"f3cfbc","input":"* detect fiducial beads in ```data/day6/alignment/timeseries/withbeads``` and use them to align 3 other timepoints of your choice","pos":11,"type":"cell"}
{"id":0,"time":1687119988669,"type":"user"}
{"last_load":1687119989137,"type":"file"}